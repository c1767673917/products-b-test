#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é£ä¹¦å¤šç»´è¡¨æ ¼æ•°æ®åˆ†æå™¨
ç”¨äºè·å–ã€åˆ†æå’Œå¯¼å‡ºé£ä¹¦å¤šç»´è¡¨æ ¼æ•°æ®
"""

import requests
import json
import time
import pandas as pd
from datetime import datetime
from typing import Optional, Dict, Any, List
import os

class FeishuDataAnalyzer:
    """é£ä¹¦å¤šç»´è¡¨æ ¼æ•°æ®åˆ†æå™¨"""
    
    def __init__(self, app_id: str, app_secret: str):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.app_id = app_id
        self.app_secret = app_secret
        self.base_url = "https://open.feishu.cn"
        self.access_token = None
        self.token_expires_at = 0
    
    def get_tenant_access_token(self) -> Optional[str]:
        """è·å–ç§Ÿæˆ·è®¿é—®å‡­è¯"""
        if self.access_token and time.time() < self.token_expires_at:
            return self.access_token
            
        url = f"{self.base_url}/open-apis/auth/v3/tenant_access_token/internal"
        headers = {'Content-Type': 'application/json; charset=utf-8'}
        data = {"app_id": self.app_id, "app_secret": self.app_secret}
        
        try:
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            if result.get('code') == 0:
                self.access_token = result['tenant_access_token']
                self.token_expires_at = time.time() + result.get('expire', 7200) - 300
                return self.access_token
            else:
                print(f"âŒ è·å–è®¿é—®å‡­è¯å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚é”™è¯¯: {e}")
            return None
    
    def get_all_records(self, app_token: str, table_id: str) -> List[Dict[str, Any]]:
        """è·å–è¡¨æ ¼ä¸­çš„æ‰€æœ‰è®°å½•ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
        access_token = self.get_tenant_access_token()
        if not access_token:
            return []
            
        all_records = []
        page_token = None
        page_size = 500  # æœ€å¤§é¡µé¢å¤§å°
        
        while True:
            url = f"{self.base_url}/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
            headers = {'Authorization': f'Bearer {access_token}'}
            params = {'page_size': page_size}
            
            if page_token:
                params['page_token'] = page_token
            
            try:
                response = requests.get(url, headers=headers, params=params)
                response.raise_for_status()
                
                data = response.json()
                if data.get('code') == 0:
                    records = data['data']['items']
                    all_records.extend(records)
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
                    if data['data'].get('has_more'):
                        page_token = data['data'].get('page_token')
                        print(f"ğŸ“„ å·²è·å– {len(all_records)} æ¡è®°å½•ï¼Œç»§ç»­è·å–...")
                    else:
                        break
                else:
                    print(f"âŒ è·å–è®°å½•å¤±è´¥: {data.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                    break
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ è¯·æ±‚é”™è¯¯: {e}")
                break
        
        print(f"âœ… æ€»å…±è·å–åˆ° {len(all_records)} æ¡è®°å½•")
        return all_records
    
    def flatten_record_data(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†è®°å½•æ•°æ®æ‰å¹³åŒ–ï¼Œä¾¿äºåˆ†æ"""
        flattened_records = []
        
        for record in records:
            flat_record = {'record_id': record.get('record_id')}
            fields = record.get('fields', {})
            
            for field_name, field_value in fields.items():
                # å¤„ç†ä¸åŒç±»å‹çš„å­—æ®µå€¼
                if isinstance(field_value, list):
                    if len(field_value) > 0:
                        if isinstance(field_value[0], dict):
                            # å¤„ç†å¤æ‚å¯¹è±¡åˆ—è¡¨ï¼ˆå¦‚å›¾ç‰‡ã€é“¾æ¥ç­‰ï¼‰
                            if 'text' in field_value[0]:
                                flat_record[field_name] = field_value[0]['text']
                            elif 'name' in field_value[0]:
                                flat_record[field_name] = field_value[0]['name']
                            else:
                                flat_record[field_name] = str(field_value[0])
                        else:
                            # å¤„ç†ç®€å•å€¼åˆ—è¡¨
                            flat_record[field_name] = ', '.join(map(str, field_value))
                    else:
                        flat_record[field_name] = ''
                elif isinstance(field_value, dict):
                    # å¤„ç†å­—å…¸ç±»å‹ï¼ˆå¦‚é“¾æ¥ï¼‰
                    if 'text' in field_value:
                        flat_record[field_name] = field_value['text']
                    elif 'link' in field_value:
                        flat_record[field_name] = field_value['link']
                    else:
                        flat_record[field_name] = str(field_value)
                else:
                    # å¤„ç†ç®€å•ç±»å‹
                    flat_record[field_name] = field_value
            
            flattened_records.append(flat_record)
        
        return flattened_records
    
    def analyze_data(self, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®å¹¶ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        if not records:
            return {}

        df = pd.DataFrame(records)
        analysis = {
            'total_records': len(records),
            'columns': list(df.columns),
            'column_count': len(df.columns),
            'data_types': {k: str(v) for k, v in df.dtypes.to_dict().items()},
            'missing_values': {k: int(v) for k, v in df.isnull().sum().to_dict().items()},
            'unique_values': {}
        }
        
        # åˆ†æåˆ†ç±»å­—æ®µçš„å”¯ä¸€å€¼
        categorical_fields = ['å“ç±»ä¸€çº§', 'å“ç±»äºŒçº§', 'é‡‡é›†å¹³å°', 'äº§åœ°ï¼ˆçœï¼‰', 'äº§åœ°ï¼ˆå¸‚ï¼‰', 'å•æ··']
        for field in categorical_fields:
            if field in df.columns:
                analysis['unique_values'][field] = df[field].value_counts().to_dict()
        
        # ä»·æ ¼åˆ†æ
        price_fields = ['æ­£å¸¸å”®ä»·', 'ä¼˜æƒ åˆ°æ‰‹ä»·']
        for field in price_fields:
            if field in df.columns:
                # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                numeric_values = pd.to_numeric(df[field], errors='coerce').dropna()
                if len(numeric_values) > 0:
                    analysis[f'{field}_stats'] = {
                        'count': len(numeric_values),
                        'mean': round(numeric_values.mean(), 2),
                        'median': round(numeric_values.median(), 2),
                        'min': round(numeric_values.min(), 2),
                        'max': round(numeric_values.max(), 2)
                    }
        
        return analysis
    
    def save_to_files(self, records: List[Dict[str, Any]], analysis: Dict[str, Any], 
                     app_token: str) -> None:
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = f"feishu_data_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜åŸå§‹JSONæ•°æ®
        with open(f"{output_dir}/raw_data.json", 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‰å¹³åŒ–æ•°æ®ä¸ºCSV
        flattened_data = self.flatten_record_data(records)
        df = pd.DataFrame(flattened_data)
        df.to_csv(f"{output_dir}/data.csv", index=False, encoding='utf-8-sig')
        
        # ä¿å­˜åˆ†ææŠ¥å‘Š
        with open(f"{output_dir}/analysis_report.json", 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆå¯è¯»çš„åˆ†ææŠ¥å‘Š
        self.generate_readable_report(analysis, f"{output_dir}/analysis_report.txt")
        
        print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°ç›®å½•: {output_dir}")
        print(f"   ğŸ“„ raw_data.json - åŸå§‹JSONæ•°æ®")
        print(f"   ğŸ“Š data.csv - æ‰å¹³åŒ–CSVæ•°æ®")
        print(f"   ğŸ“ˆ analysis_report.json - åˆ†ææŠ¥å‘Š(JSON)")
        print(f"   ğŸ“ analysis_report.txt - åˆ†ææŠ¥å‘Š(å¯è¯»)")
    
    def generate_readable_report(self, analysis: Dict[str, Any], filepath: str) -> None:
        """ç”Ÿæˆå¯è¯»çš„åˆ†ææŠ¥å‘Š"""
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("é£ä¹¦å¤šç»´è¡¨æ ¼æ•°æ®åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n")
            f.write(f"æ€»è®°å½•æ•°: {analysis.get('total_records', 0)}\n")
            f.write(f"å­—æ®µæ•°é‡: {analysis.get('column_count', 0)}\n\n")
            
            f.write(f"ğŸ“‹ å­—æ®µåˆ—è¡¨\n")
            for i, col in enumerate(analysis.get('columns', []), 1):
                f.write(f"{i:2d}. {col}\n")
            f.write("\n")
            
            # åˆ†ç±»å­—æ®µç»Ÿè®¡
            f.write(f"ğŸ·ï¸ åˆ†ç±»å­—æ®µç»Ÿè®¡\n")
            for field, values in analysis.get('unique_values', {}).items():
                f.write(f"\n{field}:\n")
                for value, count in values.items():
                    f.write(f"  - {value}: {count}æ¡\n")
            
            # ä»·æ ¼ç»Ÿè®¡
            f.write(f"\nğŸ’° ä»·æ ¼ç»Ÿè®¡\n")
            for key, stats in analysis.items():
                if key.endswith('_stats'):
                    field_name = key.replace('_stats', '')
                    f.write(f"\n{field_name}:\n")
                    f.write(f"  - æœ‰æ•ˆæ•°æ®: {stats['count']}æ¡\n")
                    f.write(f"  - å¹³å‡ä»·æ ¼: Â¥{stats['mean']}\n")
                    f.write(f"  - ä¸­ä½æ•°: Â¥{stats['median']}\n")
                    f.write(f"  - æœ€ä½ä»·: Â¥{stats['min']}\n")
                    f.write(f"  - æœ€é«˜ä»·: Â¥{stats['max']}\n")


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®ä¿¡æ¯
    APP_ID = "cli_a8e575c35763d013"
    APP_SECRET = "41VyUJHWqFBoiOr5dOwgqctKwSn1RqWf"
    APP_TOKEN = "S5yvbq5txacTZOsHGszc3RpDn4f"
    TABLE_ID = "tblyBL4HSF2GqSi4"
    
    print("ğŸš€ å¼€å§‹é£ä¹¦å¤šç»´è¡¨æ ¼æ•°æ®åˆ†æ...")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = FeishuDataAnalyzer(APP_ID, APP_SECRET)
    
    # è·å–æ‰€æœ‰è®°å½•
    print("\nğŸ“¥ è·å–æ•°æ®...")
    records = analyzer.get_all_records(APP_TOKEN, TABLE_ID)
    
    if records:
        # åˆ†ææ•°æ®
        print("\nğŸ“Š åˆ†ææ•°æ®...")
        analysis = analyzer.analyze_data(analyzer.flatten_record_data(records))
        
        # æ˜¾ç¤ºç®€è¦åˆ†æç»“æœ
        print(f"\nğŸ“ˆ åˆ†æç»“æœæ¦‚è§ˆ:")
        print(f"   æ€»è®°å½•æ•°: {analysis.get('total_records', 0)}")
        print(f"   å­—æ®µæ•°é‡: {analysis.get('column_count', 0)}")
        
        # ä¿å­˜æ•°æ®
        print("\nğŸ’¾ ä¿å­˜æ•°æ®...")
        analyzer.save_to_files(records, analysis, APP_TOKEN)
        
        print("\nâœ¨ æ•°æ®åˆ†æå®Œæˆ!")
    else:
        print("âŒ æœªèƒ½è·å–åˆ°æ•°æ®")


if __name__ == "__main__":
    main()
